{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1580cb67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BertForSequenceClassification' from 'transformers' (E:\\Anaconda\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BertForSequenceClassification' from 'transformers' (E:\\Anaconda\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç†\n",
    "def load_and_clean_data(filepath):\n",
    "    \"\"\"åŠ è½½å¹¶æ¸…æ´—è¯„è®ºæ•°æ®\"\"\"\n",
    "    df = pd.read_csv(netease_comments_167827.csv, encoding='utf-8')\n",
    "    print(f\"åŸå§‹æ•°æ®é‡: {len(df)}æ¡\")\n",
    "    \n",
    "    # åŸºç¡€æ¸…æ´—\n",
    "    df = df.dropna(subset=['content'])  # åˆ é™¤ç©ºè¯„è®º\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', str(x)))  # å»æ ‡ç‚¹\n",
    "    df['content'] = df['content'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())  # å»ç©ºæ ¼\n",
    "    \n",
    "    # ä¸­æ–‡åˆ†è¯\n",
    "    print(\"æ­£åœ¨è¿›è¡Œä¸­æ–‡åˆ†è¯...\")\n",
    "    tqdm.pandas(desc=\"åˆ†è¯è¿›åº¦\")\n",
    "    df['seg_content'] = df['content'].progress_apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 2. æƒ…æ„Ÿåˆ†ææ¨¡å‹\n",
    "class SentimentAnalyzer:\n",
    "    def __init__(self, model_name=\"bert-base-chinese\"):\n",
    "        \"\"\"åˆå§‹åŒ–BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹\"\"\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "        self.label_map = {0: \"è´Ÿé¢\", 1: \"ä¸­æ€§\", 2: \"æ­£é¢\"}\n",
    "        \n",
    "    def predict(self, texts, batch_size=16):\n",
    "        \"\"\"æ‰¹é‡é¢„æµ‹æ–‡æœ¬æƒ…æ„Ÿ\"\"\"\n",
    "        results = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"æƒ…æ„Ÿåˆ†æè¿›åº¦\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(\n",
    "                batch, \n",
    "                padding=True, \n",
    "                truncation=True, \n",
    "                max_length=128, \n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            results.extend(preds.numpy())\n",
    "        return [self.label_map[p] for p in results]\n",
    "\n",
    "# 3. å¯è§†åŒ–åˆ†æ\n",
    "def visualize_results(df):\n",
    "    \"\"\"ç”Ÿæˆåˆ†æå›¾è¡¨\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%')\n",
    "    plt.title('æƒ…æ„Ÿåˆ†å¸ƒæ¯”ä¾‹')\n",
    "    \n",
    "    # è¯äº‘ç”Ÿæˆ\n",
    "    plt.subplot(2, 2, 2)\n",
    "    text = ' '.join(df['seg_content'])\n",
    "    wordcloud = WordCloud(\n",
    "        font_path='SimHei.ttf', \n",
    "        width=800, \n",
    "        height=600, \n",
    "        background_color='white'\n",
    "    ).generate(text)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.title('é«˜é¢‘è¯äº‘')\n",
    "    \n",
    "    # ç‚¹èµä¸æƒ…æ„Ÿå…³ç³»\n",
    "    plt.subplot(2, 2, 3)\n",
    "    df.boxplot(column='like_count', by='sentiment', showfliers=False)\n",
    "    plt.title('ä¸åŒæƒ…æ„Ÿçš„ç‚¹èµæ•°åˆ†å¸ƒ')\n",
    "    plt.suptitle('')\n",
    "    plt.xlabel('æƒ…æ„Ÿç±»å‹')\n",
    "    \n",
    "    # æ—¶é—´è¶‹åŠ¿åˆ†æï¼ˆå¦‚æœæœ‰æ—¶é—´å­—æ®µï¼‰\n",
    "    if 'time' in df.columns:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        df['date'] = pd.to_datetime(df['time']).dt.date\n",
    "        daily_sentiment = df.groupby(['date', 'sentiment']).size().unstack()\n",
    "        daily_sentiment.plot(kind='line')\n",
    "        plt.title('æƒ…æ„Ÿè¶‹åŠ¿å˜åŒ–')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. ä¸»æµç¨‹\n",
    "def main():\n",
    "    # åŠ è½½æ•°æ®ï¼ˆæ›¿æ¢ä¸ºä½ çš„æ–‡ä»¶è·¯å¾„ï¼‰\n",
    "    file_path = \"netease_comments_167827.csv\"  # ç¡®ä¿åŒ…å«contentå­—æ®µ\n",
    "    df = load_and_clean_data(file_path)\n",
    "    \n",
    "    # æƒ…æ„Ÿåˆ†æï¼ˆä½¿ç”¨é¢„è®­ç»ƒBERTæ¨¡å‹ï¼‰\n",
    "    print(\"\\næ­£åœ¨è¿›è¡Œæƒ…æ„Ÿåˆ†æ...\")\n",
    "    analyzer = SentimentAnalyzer()\n",
    "    sample_size = min(1000, len(df))  # é™åˆ¶åˆ†ææ•°é‡ï¼ˆå¯é€‰ï¼‰\n",
    "    df_sample = df.sample(sample_size, random_state=42) if sample_size < len(df) else df\n",
    "    df_sample['sentiment'] = analyzer.predict(df_sample['content'].tolist())\n",
    "    \n",
    "    # ç»“æœåˆ†æ\n",
    "    print(\"\\næƒ…æ„Ÿåˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(\n",
    "        df_sample['sentiment'], \n",
    "        df_sample['sentiment'], \n",
    "        target_names=['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢']\n",
    "    ))\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    visualize_results(df_sample)\n",
    "    \n",
    "    # ä¿å­˜ç»“æœ\n",
    "    output_path = \"sentiment_analysis_results.csv\"\n",
    "    df_sample.to_csv(output_path, index=False, encoding='utf_8_sig')\n",
    "    print(f\"\\nåˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_path}\")\n",
    "\n",
    "# è¿è¡Œä¸»ç¨‹åº\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0de50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… pandas                         å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 1.5.3\n",
      "âœ… numpy                          å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 1.24.3\n",
      "âœ… jieba                          å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 0.42.1\n",
      "âœ… sklearn.model_selection        å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 1.3.0\n",
      "âœ… sklearn.metrics                å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 1.3.0\n",
      "âœ… matplotlib.pyplot              å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 3.7.1\n",
      "âœ… wordcloud                      å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 1.9.4\n",
      "âœ… transformers                   å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 2.1.1\n",
      "âœ… transformers                   å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 2.1.1\n",
      "âŒ torch                          æœªå®‰è£…\n",
      "âœ… tqdm                           å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: 4.65.0\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥æ¯ä¸ªåº“æ˜¯å¦èƒ½æ­£å¸¸å¯¼å…¥\n",
    "libs = [\n",
    "    ('pandas', 'pd'),\n",
    "    ('numpy', 'np'),\n",
    "    ('jieba', 'jieba'),\n",
    "    ('sklearn.model_selection', 'train_test_split'),\n",
    "    ('sklearn.metrics', 'classification_report'),\n",
    "    ('matplotlib.pyplot', 'plt'),\n",
    "    ('wordcloud', 'WordCloud'),\n",
    "    ('transformers', 'BertTokenizer'),\n",
    "    ('transformers', 'BertForSequenceClassification'),\n",
    "    ('torch', 'torch'),\n",
    "    ('tqdm', 'tqdm')\n",
    "]\n",
    "\n",
    "for lib, alias in libs:\n",
    "    try:\n",
    "        exec(f\"import {lib.split('.')[0]} as {alias}\")\n",
    "        version = eval(f\"{alias}.__version__\") if hasattr(eval(alias), '__version__') else \"ç‰ˆæœ¬ä¿¡æ¯ä¸å¯ç”¨\"\n",
    "        print(f\"âœ… {lib: <30} å¯¼å…¥æˆåŠŸ | ç‰ˆæœ¬: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {lib: <30} æœªå®‰è£…\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ {lib: <30} å¯¼å…¥é”™è¯¯: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33b5c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…å¿…è¦çš„åº“\n",
    "!pip install snownlp wordcloud jieba\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from snownlp import SnowNLP\n",
    "from wordcloud import WordCloud\n",
    "import jieba\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†æå‡½æ•°\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        s = SnowNLP(text)\n",
    "        return s.sentiments\n",
    "    except:\n",
    "        return 0.5  # ä¸­æ€§ä½œä¸ºé»˜è®¤å€¼\n",
    "\n",
    "# ç”Ÿæˆè¯äº‘å‡½æ•°\n",
    "def generate_wordcloud(texts, title):\n",
    "    word_list = []\n",
    "    for text in texts:\n",
    "        words = jieba.cut(text)\n",
    "        word_list.extend([word for word in words if len(word) > 1 and word not in ['å“ˆå“ˆ', 'å•Šå•Š', 'å‘œå‘œ']])\n",
    "    \n",
    "    wordcloud = WordCloud(\n",
    "        font_path='SimHei.ttf',\n",
    "        background_color='white',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        max_words=100\n",
    "    ).generate(' '.join(word_list))\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†æ\n",
    "display(Markdown(\"## ğŸ­ æƒ…æ„Ÿåˆ†æ\"))\n",
    "df['æƒ…æ„Ÿåˆ†å€¼'] = df['è¯„è®ºå†…å®¹'].apply(analyze_sentiment)\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†å¸ƒå¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['æƒ…æ„Ÿåˆ†å€¼'], bins=20, kde=True)\n",
    "plt.title('è¯„è®ºæƒ…æ„Ÿåˆ†å€¼åˆ†å¸ƒ')\n",
    "plt.xlabel('æƒ…æ„Ÿåˆ†å€¼ (0-1, è¶Šæ¥è¿‘1è¡¨ç¤ºè¶Šç§¯æ)')\n",
    "plt.ylabel('è¯„è®ºæ•°é‡')\n",
    "plt.show()\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†ç±»\n",
    "df['æƒ…æ„Ÿåˆ†ç±»'] = pd.cut(df['æƒ…æ„Ÿåˆ†å€¼'], \n",
    "                      bins=[0, 0.3, 0.7, 1],\n",
    "                      labels=['è´Ÿé¢', 'ä¸­æ€§', 'æ­£é¢'])\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†ç±»ç»Ÿè®¡\n",
    "sentiment_stats = df['æƒ…æ„Ÿåˆ†ç±»'].value_counts(normalize=True).mul(100).round(1)\n",
    "display(Markdown(\"### æƒ…æ„Ÿåˆ†ç±»æ¯”ä¾‹ (%)\"))\n",
    "display(sentiment_stats)\n",
    "\n",
    "# æƒ…æ„Ÿåˆ†ç±»é¥¼å›¾\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sentiment_stats, labels=sentiment_stats.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('è¯„è®ºæƒ…æ„Ÿåˆ†ç±»æ¯”ä¾‹')\n",
    "plt.show()\n",
    "\n",
    "# ç‚¹èµæ•°ä¸æƒ…æ„Ÿå…³ç³»\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='æƒ…æ„Ÿåˆ†å€¼', y='ç‚¹èµæ•°', alpha=0.6)\n",
    "plt.title('æƒ…æ„Ÿåˆ†å€¼ä¸ç‚¹èµæ•°çš„å…³ç³»')\n",
    "plt.show()\n",
    "\n",
    "# ç”¨æˆ·ç”»åƒ - æ´»è·ƒç”¨æˆ·\n",
    "top_users = df['ç”¨æˆ·'].value_counts().head(5)\n",
    "display(Markdown(\"### ğŸ† æœ€æ´»è·ƒçš„5ä½ç”¨æˆ· (è¯„è®ºæ¬¡æ•°æœ€å¤š)\"))\n",
    "display(top_users)\n",
    "\n",
    "# ç”¨æˆ·ç”»åƒ - é«˜å½±å“åŠ›ç”¨æˆ·\n",
    "top_influencers = df.sort_values('ç‚¹èµæ•°', ascending=False).drop_duplicates('ç”¨æˆ·ID').head(5)[['ç”¨æˆ·', 'ç‚¹èµæ•°', 'è¯„è®ºå†…å®¹']]\n",
    "display(Markdown(\"### ğŸ’ª æœ€å…·å½±å“åŠ›çš„5ä½ç”¨æˆ· (å•æ¡è¯„è®ºç‚¹èµæœ€å¤š)\"))\n",
    "display(top_influencers)\n",
    "\n",
    "# è¯äº‘åˆ†æ - å…¨éƒ¨è¯„è®º\n",
    "display(Markdown(\"### â˜ å…¨éƒ¨è¯„è®ºè¯äº‘\"))\n",
    "generate_wordcloud(df['è¯„è®ºå†…å®¹'], 'å…¨éƒ¨è¯„è®ºè¯äº‘')\n",
    "\n",
    "# è¯äº‘åˆ†æ - æ­£é¢è¯„è®º\n",
    "display(Markdown(\"### ğŸ˜Š æ­£é¢è¯„è®ºè¯äº‘ (æƒ…æ„Ÿåˆ†å€¼ > 0.7)\"))\n",
    "positive_comments = df[df['æƒ…æ„Ÿåˆ†å€¼'] > 0.7]['è¯„è®ºå†…å®¹']\n",
    "generate_wordcloud(positive_comments, 'æ­£é¢è¯„è®ºè¯äº‘')\n",
    "\n",
    "# è¯äº‘åˆ†æ - è´Ÿé¢è¯„è®º\n",
    "display(Markdown(\"### ğŸ˜ è´Ÿé¢è¯„è®ºè¯äº‘ (æƒ…æ„Ÿåˆ†å€¼ < 0.3)\"))\n",
    "negative_comments = df[df['æƒ…æ„Ÿåˆ†å€¼'] < 0.3]['è¯„è®ºå†…å®¹']\n",
    "generate_wordcloud(negative_comments, 'è´Ÿé¢è¯„è®ºè¯äº‘')\n",
    "\n",
    "# IPå±åœ°åˆ†æ\n",
    "if 'IPå±åœ°' in df.columns:\n",
    "    display(Markdown(\"### ğŸŒ ç”¨æˆ·åœ°åŸŸåˆ†å¸ƒ\"))\n",
    "    location_stats = df['IPå±åœ°'].value_counts().head(10)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    location_stats.plot(kind='bar')\n",
    "    plt.title('è¯„è®ºç”¨æˆ·Top10åœ°åŸŸåˆ†å¸ƒ')\n",
    "    plt.xlabel('åœ°åŸŸ')\n",
    "    plt.ylabel('è¯„è®ºæ•°é‡')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# æ—¶é—´åˆ†æ\n",
    "display(Markdown(\"### â° è¯„è®ºæ—¶é—´åˆ†å¸ƒ\"))\n",
    "df['å°æ—¶'] = df['è¯„è®ºæ—¶é—´'].dt.hour\n",
    "hourly_counts = df['å°æ—¶'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "hourly_counts.plot(kind='bar')\n",
    "plt.title('24å°æ—¶è¯„è®ºæ•°é‡åˆ†å¸ƒ')\n",
    "plt.xlabel('å°æ—¶')\n",
    "plt.ylabel('è¯„è®ºæ•°é‡')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# ä¿å­˜åˆ†æç»“æœ\n",
    "analysis_file = f'netease_analysis_{song_id}.csv'\n",
    "df.to_csv(analysis_file, index=False, encoding='utf-8-sig')\n",
    "display(Markdown(f\"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜ä¸º {analysis_file}\"))\n",
    "\n",
    "# æ€»ç»“æŠ¥å‘Š\n",
    "display(Markdown(\"## ğŸ“ åˆ†ææ€»ç»“æŠ¥å‘Š\"))\n",
    "display(Markdown(f\"\"\"\n",
    "1. **æƒ…æ„Ÿåˆ†å¸ƒ**: æ­Œæ›²è¯„è®ºä¸­æ­£é¢æƒ…æ„Ÿå æ¯” {sentiment_stats.get('æ­£é¢', 0)}%ï¼Œè´Ÿé¢æƒ…æ„Ÿå æ¯” {sentiment_stats.get('è´Ÿé¢', 0)}%\n",
    "2. **æ´»è·ƒæ—¶é—´**: è¯„è®ºé«˜å³°æ—¶æ®µä¸º {hourly_counts.idxmax()}:00 æ—¶ï¼Œå…± {hourly_counts.max()} æ¡è¯„è®º\n",
    "3. **çƒ­é—¨åœ°åŸŸ**: è¯„è®ºç”¨æˆ·ä¸»è¦æ¥è‡ª {location_stats.index[0] if 'IPå±åœ°' in df.columns else 'æœªçŸ¥'}\n",
    "4. **å†…å®¹ç‰¹ç‚¹**: é«˜é¢‘è¯æ±‡å¯é€šè¿‡è¯äº‘å›¾ç›´è§‚æŸ¥çœ‹ï¼Œåæ˜ äº†å¬ä¼—çš„ä¸»è¦å…³æ³¨ç‚¹\n",
    "5. **ç”¨æˆ·äº’åŠ¨**: æœ€é«˜ç‚¹èµè¯„è®ºè·å¾— {df['ç‚¹èµæ•°'].max()} ä¸ªèµï¼Œå†…å®¹ä¸º \"{df.sort_values('ç‚¹èµæ•°', ascending=False).iloc[0]['è¯„è®ºå†…å®¹'][:30]}...\"\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b217730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æƒ…æ„Ÿæ¨¡å‹ä¼˜åŒ–ä¸æ™ºèƒ½æ¨èç³»ç»Ÿ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from snownlp import SnowNLP\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from gensim.models import Word2Vec\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 1. æ•°æ®å‡†å¤‡ï¼ˆæ¥ç»­ä¹‹å‰çˆ¬å–çš„è¯„è®ºæ•°æ®ï¼‰\n",
    "df = pd.read_csv(f'netease_comments_{song_id}.csv')\n",
    "print(f\"åŸå§‹æ•°æ®é‡ï¼š{len(df)}æ¡\")\n",
    "\n",
    "# 2. æƒ…æ„Ÿæ¨¡å‹ä¼˜åŒ–ï¼ˆç»“åˆé¢†åŸŸè¯å…¸å’ŒåŠ æƒæœºåˆ¶ï¼‰\n",
    "class EnhancedSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # åŠ è½½éŸ³ä¹é¢†åŸŸæƒ…æ„Ÿè¯å…¸\n",
    "        self.music_lexicon = {\n",
    "            'æ—‹å¾‹': 0.8, 'ç¼–æ›²': 0.7, 'ä½œè¯': 0.6, \n",
    "            'é’æ˜¥': 0.9, 'å›å¿†': 0.85, 'ç»å…¸': 0.75,\n",
    "            'éš¾å¬': -0.9, 'è¿‡æ—¶': -0.6, 'å¤±æœ›': -0.7\n",
    "        }\n",
    "        self.base_analyzer = SnowNLP\n",
    "        \n",
    "    def analyze(self, text):\n",
    "        try:\n",
    "            # åŸºç¡€æƒ…æ„Ÿåˆ†æ\n",
    "            s = self.base_analyzer(text)\n",
    "            base_score = s.sentiments\n",
    "            \n",
    "            # é¢†åŸŸè¯å…¸å¢å¼º\n",
    "            words = jieba.lcut(text)\n",
    "            lexicon_score = np.mean([self.music_lexicon.get(word, 0) for word in words])\n",
    "            \n",
    "            # æ—¶é—´åŠ æƒï¼ˆå¤œé—´è¯„è®ºæƒ…æ„Ÿæ”¾å¤§ï¼‰\n",
    "            hour = pd.to_datetime(df[df['è¯„è®ºå†…å®¹'] == text]['è¯„è®ºæ—¶é—´'].iloc[0]).hour\n",
    "            time_weight = 1.2 if 0 <= hour < 6 else 1.0\n",
    "            \n",
    "            # ç»¼åˆè®¡ç®—ï¼ˆåŸºç¡€åˆ†60% + é¢†åŸŸåˆ†40%ï¼‰\n",
    "            final_score = (base_score*0.6 + np.tanh(lexicon_score)*0.4) * time_weight\n",
    "            return np.clip(final_score, 0, 1)\n",
    "        except:\n",
    "            return 0.5\n",
    "\n",
    "# åº”ç”¨ä¼˜åŒ–åçš„æƒ…æ„Ÿåˆ†æ\n",
    "analyzer = EnhancedSentimentAnalyzer()\n",
    "df['ä¼˜åŒ–æƒ…æ„Ÿåˆ†'] = df['è¯„è®ºå†…å®¹'].progress_apply(analyzer.analyze)\n",
    "\n",
    "# å¯¹æ¯”ä¼˜åŒ–æ•ˆæœ\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(121)\n",
    "df['æƒ…æ„Ÿåˆ†å€¼'].hist(bins=20)\n",
    "plt.title('åŸå§‹æƒ…æ„Ÿåˆ†å¸ƒ')\n",
    "plt.subplot(122)\n",
    "df['ä¼˜åŒ–æƒ…æ„Ÿåˆ†'].hist(bins=20)\n",
    "plt.title('ä¼˜åŒ–åæƒ…æ„Ÿåˆ†å¸ƒ')\n",
    "plt.show()\n",
    "\n",
    "# 3. ç”¨æˆ·å¿ƒç†åˆ†æï¼ˆLDAä¸»é¢˜æ¨¡å‹ï¼‰\n",
    "def analyze_psychology(texts):\n",
    "    # ä¸­æ–‡åˆ†è¯å¤„ç†\n",
    "    chinese_stopwords = set(['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'æˆ‘'])\n",
    "    processed_texts = [\n",
    "        [word for word in jieba.lcut(text) \n",
    "         if len(word) > 1 and word not in chinese_stopwords]\n",
    "        for text in texts\n",
    "    ]\n",
    "    \n",
    "    # è®­ç»ƒWord2Vecæ¨¡å‹\n",
    "    w2v_model = Word2Vec(processed_texts, vector_size=100, window=5, min_count=2, workers=4)\n",
    "    \n",
    "    # TF-IDFå‘é‡åŒ–\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    tfidf = vectorizer.fit_transform([' '.join(t) for t in processed_texts])\n",
    "    \n",
    "    # LDAä¸»é¢˜å»ºæ¨¡\n",
    "    lda = LatentDirichletAllocation(n_components=3, random_state=42)\n",
    "    lda.fit(tfidf)\n",
    "    \n",
    "    # å¯è§†åŒ–ä¸»é¢˜è¯\n",
    "    def show_topics(model, feature_names, n_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            display(Markdown(f\"**ä¸»é¢˜#{topic_idx+1}**: \" + \n",
    "                   \", \".join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]])))\n",
    "    \n",
    "    show_topics(lda, vectorizer.get_feature_names_out(), 10)\n",
    "    \n",
    "    return lda.transform(tfidf)\n",
    "\n",
    "# åº”ç”¨å¿ƒç†åˆ†æ\n",
    "topic_probs = analyze_psychology(df['è¯„è®ºå†…å®¹'])\n",
    "df['å¿ƒç†ä¸»é¢˜'] = topic_probs.argmax(axis=1)\n",
    "\n",
    "# 4. ç”¨æˆ·èšç±»åˆ†æ\n",
    "def user_clustering(features):\n",
    "    # ç‰¹å¾å·¥ç¨‹ï¼ˆæƒ…æ„Ÿåˆ†+ç‚¹èµæ•°+ä¸»é¢˜æ¦‚ç‡ï¼‰\n",
    "    X = np.column_stack([\n",
    "        features['ä¼˜åŒ–æƒ…æ„Ÿåˆ†'],\n",
    "        np.log1p(features['ç‚¹èµæ•°']),\n",
    "        topic_probs\n",
    "    ])\n",
    "    \n",
    "    # å¯»æ‰¾æœ€ä½³èšç±»æ•°\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, 6):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "    \n",
    "    best_k = np.argmax(silhouette_scores) + 2\n",
    "    display(Markdown(f\"### æœ€ä½³èšç±»æ•°ï¼š{best_k}ï¼ˆè½®å»“ç³»æ•°{max(silhouette_scores):.2f}ï¼‰\"))\n",
    "    \n",
    "    # æœ€ç»ˆèšç±»\n",
    "    final_kmeans = KMeans(n_clusters=best_k, random_state=42)\n",
    "    df['ç”¨æˆ·ç±»å‹'] = final_kmeans.fit_predict(X)\n",
    "    \n",
    "    # å¯è§†åŒ–èšç±»ç»“æœ\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for cluster in range(best_k):\n",
    "        cluster_data = df[df['ç”¨æˆ·ç±»å‹'] == cluster]\n",
    "        plt.scatter(\n",
    "            cluster_data['ä¼˜åŒ–æƒ…æ„Ÿåˆ†'], \n",
    "            np.log1p(cluster_data['ç‚¹èµæ•°']),\n",
    "            label=f'ç±»å‹{cluster}'\n",
    "        )\n",
    "    plt.xlabel('ä¼˜åŒ–æƒ…æ„Ÿåˆ†')\n",
    "    plt.ylabel('ç‚¹èµæ•°(log)')\n",
    "    plt.title('ç”¨æˆ·èšç±»åˆ†å¸ƒ')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # åˆ†æå„ç±»ç‰¹å¾\n",
    "    display(Markdown(\"### å„ç±»ç”¨æˆ·å¹³å‡ç‰¹å¾ï¼š\"))\n",
    "    display(df.groupby('ç”¨æˆ·ç±»å‹').agg({\n",
    "        'ä¼˜åŒ–æƒ…æ„Ÿåˆ†': 'mean',\n",
    "        'ç‚¹èµæ•°': 'median',\n",
    "        'å¿ƒç†ä¸»é¢˜': lambda x: x.mode()[0]\n",
    "    }))\n",
    "\n",
    "user_clustering(df)\n",
    "\n",
    "# 5. æ™ºèƒ½æ¨èç³»ç»Ÿ\n",
    "class MusicRecommender:\n",
    "    def __init__(self, user_data):\n",
    "        self.user_profiles = user_data\n",
    "        self.song_db = {\n",
    "            167827: {'title': 'ç´ é¢œ', 'tags': ['é’æ˜¥', 'å›å¿†', 'å¯¹å”±']},\n",
    "            186016: {'title': 'æœ‰ä½•ä¸å¯', 'tags': ['ç”œèœœ', 'åˆæ‹', 'æ ¡å›­']},\n",
    "            287035: {'title': 'é›…ä¿—å…±èµ', 'tags': ['å“²ç†', 'äººç”Ÿ', 'æ€è€ƒ']},\n",
    "            5260494: {'title': 'ä¹Œé¸¦', 'tags': ['å­¤ç‹¬', 'æˆé•¿', 'æ²»æ„ˆ']}\n",
    "        }\n",
    "        \n",
    "    def recommend(self, user_id, top_n=3):\n",
    "        # è·å–ç”¨æˆ·ç‰¹å¾\n",
    "        user = self.user_profiles[self.user_profiles['ç”¨æˆ·ID'] == user_id].iloc[0]\n",
    "        \n",
    "        # æ ¹æ®ç”¨æˆ·ç±»å‹å’Œå¿ƒç†ä¸»é¢˜åŒ¹é…æ­Œæ›²\n",
    "        if user['ç”¨æˆ·ç±»å‹'] == 0:  # é«˜æƒ…æ„Ÿ-æ€€æ—§å‹\n",
    "            candidates = [sid for sid, meta in self.song_db.items() \n",
    "                        if 'å›å¿†' in meta['tags']]\n",
    "        elif user['ç”¨æˆ·ç±»å‹'] == 1:  # ç†æ€§åˆ†æå‹\n",
    "            candidates = [sid for sid, meta in self.song_db.items() \n",
    "                        if 'æ€è€ƒ' in meta['tags']]\n",
    "        else:  # æƒ…æ„Ÿæ³¢åŠ¨å‹\n",
    "            candidates = [sid for sid, meta in self.song_db.items() \n",
    "                        if 'æ²»æ„ˆ' in meta['tags']]\n",
    "        \n",
    "        # ç»“åˆæ—¶é—´ä¸Šä¸‹æ–‡\n",
    "        hour = pd.to_datetime(user['è¯„è®ºæ—¶é—´']).hour\n",
    "        if 0 <= hour < 6:  # æ·±å¤œæ—¶æ®µæ¨èèˆ’ç¼“æ­Œæ›²\n",
    "            candidates = [sid for sid in candidates \n",
    "                         if sid not in [186016]]  # æ’é™¤ç”œèœœæ­Œæ›²\n",
    "        \n",
    "        # è¿”å›æ¨èç»“æœ\n",
    "        return [(sid, self.song_db[sid]['title']) for sid in candidates[:top_n]]\n",
    "\n",
    "# æµ‹è¯•æ¨èç³»ç»Ÿ\n",
    "recommender = MusicRecommender(df)\n",
    "sample_user = df.iloc[10]['ç”¨æˆ·ID']  # å–ç¬¬10ä¸ªç”¨æˆ·ä½œä¸ºç¤ºä¾‹\n",
    "rec_results = recommender.recommend(sample_user)\n",
    "\n",
    "display(Markdown(f\"### ä¸ºç”¨æˆ·ID {sample_user} ç”Ÿæˆçš„æ¨èï¼š\"))\n",
    "for song_id, title in rec_results:\n",
    "    display(Markdown(f\"- {title} (ID: {song_id})\"))\n",
    "\n",
    "# 6. ç³»ç»Ÿè¯„ä¼°\n",
    "def evaluate_recommendations():\n",
    "    # æ¨¡æ‹Ÿè¯„ä¼°ï¼ˆå®é™…åº”ç”¨éœ€A/Bæµ‹è¯•ï¼‰\n",
    "    user_types = df['ç”¨æˆ·ç±»å‹'].unique()\n",
    "    accuracy = {}\n",
    "    \n",
    "    for utype in user_types:\n",
    "        users = df[df['ç”¨æˆ·ç±»å‹'] == utype]['ç”¨æˆ·ID'].sample(5)\n",
    "        correct = 0\n",
    "        \n",
    "        for uid in users:\n",
    "            recs = [sid for sid, _ in recommender.recommend(uid)]\n",
    "            user_theme = df[df['ç”¨æˆ·ID'] == uid]['å¿ƒç†ä¸»é¢˜'].mode()[0]\n",
    "            \n",
    "            # ç®€å•éªŒè¯ï¼šæ¨èæ­Œæ›²æ ‡ç­¾æ˜¯å¦åŒ¹é…ç”¨æˆ·ä¸»é¢˜\n",
    "            matched = any(\n",
    "                self.song_db[sid]['tags'][0] in ['å›å¿†','é’æ˜¥'] \n",
    "                if user_theme == 0 else\n",
    "                self.song_db[sid]['tags'][0] in ['å“²ç†','æ€è€ƒ']\n",
    "                for sid in recs\n",
    "            )\n",
    "            correct += int(matched)\n",
    "        \n",
    "        accuracy[utype] = correct / 5\n",
    "    \n",
    "    display(Markdown(\"### æ¨èå‡†ç¡®ç‡è¯„ä¼°ï¼ˆæŠ½æ ·æµ‹è¯•ï¼‰ï¼š\"))\n",
    "    for utype, acc in accuracy.items():\n",
    "        display(Markdown(f\"- ç±»å‹{utype}ç”¨æˆ·ï¼š{acc*100:.1f}%\"))\n",
    "\n",
    "evaluate_recommendations()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
